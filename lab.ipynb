{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор начальных условий\n",
    "\n",
    "Я взял один и тот же датасет для классификации лекарственных средств, где можно поставить обе задачи\n",
    "\n",
    "Датасет содержит:\n",
    "- Возраст\n",
    "- Пол\n",
    "- Артериальное давление\n",
    "- Холестерин\n",
    "- Соотношение натрия к калию\n",
    "- Лекарственный препарат (название)\n",
    "\n",
    "Задача классификации заключается в предсказании класса лекарства **`(Drug)`** на основе набора признаков (`Age`, `Sex`, `BP`, `Cholesterol`, `Na_to_K`). Здесь целевая переменная **`Drug`** категориальная, что делает задачу классификацией\n",
    "\n",
    "*Почему актуальна?*\n",
    "\n",
    "В реальной медицинской практике врачи часто сталкиваются с задачей выбора подходящего лекарства для лечения пациента на основе его физиологических характеристик (как и на удивление в представленном датасете - возраст, пол, давление и уровень холестерина)\n",
    "\n",
    "**Метрики:**\n",
    "1) `Accuracy`: показывает долю правильно классифицированных объектов, поэтому подходит для сбалансированных данных\n",
    "2) `F1-Score`: учитывает баланс между точностью и полнотой, что важно при несбалансированных данных\n",
    "\n",
    "Задача регрессии заключается в предсказании уровня **`Na_to_K`** (соотношение натрия к калию в крови). Это непрерывная переменная, что делает задачу регрессией\n",
    "\n",
    "*Почему актуальна?*\n",
    "\n",
    "Уровень `Na_to_K` (соотношение натрия к калию) в крови — важный показатель, который врачи используют для диагностики различных заболеваний, таких как гипертензия, почечные нарушения или сердечно-сосудистые заболевания. Это значение часто зависит от возраста, пола, состояния артериального давления и уровня холестерина\n",
    "\n",
    "**Метрика:**\n",
    "1) `Mean Squared Error (MSE)`: среднеквадратичная ошибка, чувствительная к большим отклонениям\n",
    "2) `Mean Absolute Error (MAE)`: средняя абсолютная ошибка, менее чувствительна к выбросам\n",
    "3) `R^2 (коэффициент детерминации)`: оценивает долю дисперсии, объясняемую моделью, что позволяет понять, насколько хорошо модель объясняет данные\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем датасет, выводим информацию о нём, потом преобразуем категориальные признаки в числовые\n",
    "\n",
    "Выбираем признаки и целевые переменные на основе того, какие задачи мы решаем. Разделяем данных на тренировочную и тестовую выборки и нормируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Sex      BP Cholesterol  Na_to_K   Drug\n",
      "0   23   F    HIGH        HIGH   25.355  DrugY\n",
      "1   47   M     LOW        HIGH   13.093  drugC\n",
      "2   47   M     LOW        HIGH   10.114  drugC\n",
      "3   28   F  NORMAL        HIGH    7.798  drugX\n",
      "4   61   F     LOW        HIGH   18.043  DrugY\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Age          200 non-null    int64  \n",
      " 1   Sex          200 non-null    object \n",
      " 2   BP           200 non-null    object \n",
      " 3   Cholesterol  200 non-null    object \n",
      " 4   Na_to_K      200 non-null    float64\n",
      " 5   Drug         200 non-null    object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 9.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "data = pd.read_csv('./drug200.csv')\n",
    "\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex']) # 0 - Female, 1 - Male\n",
    "data['BP'] = label_encoder.fit_transform(data['BP'])   # 0 - HIGH, 1 - LOW, 2 - NORMAL\n",
    "data['Cholesterol'] = label_encoder.fit_transform(data['Cholesterol']) # 0 - HIGH, 1 - NORMAL\n",
    "data['Drug'] = label_encoder.fit_transform(data['Drug'])\n",
    "\n",
    "X_class = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']]\n",
    "y_class = data['Drug']\n",
    "X_reg = data[['Age', 'Sex', 'BP', 'Cholesterol']]\n",
    "y_reg = data['Na_to_K']\n",
    "\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_class_train_scaled = scaler.fit_transform(X_class_train)\n",
    "X_class_test_scaled = scaler.transform(X_class_test)\n",
    "X_reg_train_scaled = scaler.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler.transform(X_reg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №1 (Проведение исследований с алгоритмом KNN)\n",
    "\n",
    "**3a. Формулировка гипотез**\n",
    "1. Использовать разные значения параметра n_neighbors\n",
    "2. Добавить новые признаки: взаимодействие между Age и Na_to_K\n",
    "3. Проверить влияние удаления выбросов на качество модели\n",
    "\n",
    "**3g. Выводы**\n",
    "1. Подбор гиперпараметров и добавление нового признака улучшили качество моделей\n",
    "2. Улучшенная модель классификации достигает более высокой точности и F1-Score\n",
    "3. Улучшенная модель регрессии показывает снижение ошибок MSE и повышение R^2 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации:\n",
      "Accuracy: 0.88\n",
      "F1-Score: 0.87\n",
      "\n",
      "Метрики для регрессии:\n",
      "MSE: 60.72\n",
      "MAE: 6.63\n",
      "R² Score: -0.21\n",
      "Лучший n_neighbors для классификации: {'n_neighbors': 1}\n",
      "Лучший n_neighbors для регрессии: {'n_neighbors': 3}\n",
      "\n",
      "Метрики для улучшенной классификации:\n",
      "Accuracy: 0.93\n",
      "F1-Score: 0.93\n",
      "\n",
      "Метрики для улучшенной регрессии:\n",
      "MSE: 17.35\n",
      "MAE: 3.21\n",
      "R² Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "knn_class = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_class.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred = knn_class.predict(X_class_test_scaled)\n",
    "\n",
    "print(\"Метрики для классификации:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred, average='weighted'):.2f}\")\n",
    "\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred = knn_reg.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"\\nМетрики для регрессии:\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred):.2f}\")\n",
    "\n",
    "# проверка гипотез - добавляем новый признак для классификации и регрессии\n",
    "data['Age_Na_to_K'] = data['Age'] * data['Na_to_K']\n",
    "X_class = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Age_Na_to_K']]\n",
    "X_reg = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Age_Na_to_K']]\n",
    "\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_class_train_scaled = scaler.fit_transform(X_class_train)\n",
    "X_class_test_scaled = scaler.transform(X_class_test)\n",
    "X_reg_train_scaled = scaler.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler.transform(X_reg_test)\n",
    "\n",
    "# подбор гиперпараметра n_neighbors для классификации\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "grid_search_class = GridSearchCV(KNeighborsClassifier(), param_grid, scoring='f1_weighted', cv=5)\n",
    "grid_search_class.fit(X_class_train_scaled, y_class_train)\n",
    "print(f\"Лучший n_neighbors для классификации: {grid_search_class.best_params_}\")\n",
    "\n",
    "# подбор гиперпараметра n_neighbors для регрессии\n",
    "grid_search_reg = GridSearchCV(KNeighborsRegressor(), param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "print(f\"Лучший n_neighbors для регрессии: {grid_search_reg.best_params_}\")\n",
    "\n",
    "knn_class_best = grid_search_class.best_estimator_\n",
    "knn_class_best.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred_best = knn_class_best.predict(X_class_test_scaled)\n",
    "\n",
    "knn_reg_best = grid_search_reg.best_estimator_\n",
    "knn_reg_best.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred_best = knn_reg_best.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"\\nМетрики для улучшенной классификации:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_best):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_best, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для улучшенной регрессии:\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_best):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для кастомной классификации:\n",
      "Accuracy: 0.93\n",
      "F1-Score: 0.92\n",
      "\n",
      "Метрики для кастомной регрессии:\n",
      "MSE: 20.36\n",
      "MAE: 3.60\n",
      "R² Score: 0.59\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class CustomKNN:\n",
    "    def __init__(self, n_neighbors=5, mode='classification'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.mode = mode\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        predictions = [self._predict_single(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "        nearest_indices = distances.argsort()[:self.n_neighbors]\n",
    "        nearest_labels = self.y_train[nearest_indices]\n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            most_common = Counter(nearest_labels).most_common(1)\n",
    "            return most_common[0][0]\n",
    "        elif self.mode == 'regression':\n",
    "            return nearest_labels.mean()\n",
    "        \n",
    "\n",
    "knn_class_custom = CustomKNN(n_neighbors=5, mode='classification')\n",
    "knn_class_custom.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred_custom = knn_class_custom.predict(X_class_test_scaled)\n",
    "\n",
    "print(\"Метрики для кастомной классификации:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_custom):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_custom, average='weighted'):.2f}\")\n",
    "\n",
    "knn_reg_custom = CustomKNN(n_neighbors=5, mode='regression')\n",
    "knn_reg_custom.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred_custom = knn_reg_custom.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"\\nМетрики для кастомной регрессии:\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_custom):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_custom):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_custom):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №2 (Проведение исследований с логистической и линейной регрессией) \n",
    "\n",
    "**3a. Формулирование гипотез**\n",
    "1. Проверить влияние новых признаков\n",
    "2. Подобрать гиперпараметры модели с использованием кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации (бейзлайн):\n",
      "Accuracy: 0.93\n",
      "F1-Score: 0.93\n",
      "\n",
      "Метрики для регрессии (бейзлайн):\n",
      "MSE: 6.71\n",
      "MAE: 2.11\n",
      "R² Score: 0.87\n",
      "Лучший параметр C для логистической регрессии: {'C': 100}\n",
      "Лучший параметр fit_intercept для линейной регрессии: {'fit_intercept': True}\n",
      "\n",
      "Метрики для классификации (улучшенный бейзлайн):\n",
      "Accuracy: 0.97\n",
      "F1-Score: 0.98\n",
      "\n",
      "Метрики для регрессии (улучшенный бейзлайн):\n",
      "MSE: 6.45\n",
      "MAE: 2.03\n",
      "R² Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred = log_reg.predict(X_class_test_scaled)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred = lin_reg.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"Метрики для классификации (бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred):.2f}\")\n",
    "\n",
    "\n",
    "data['Age_Na_to_K'] = data['Age'] * data['Na_to_K']\n",
    "X_class = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Age_Na_to_K']]\n",
    "X_reg = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Age_Na_to_K']]\n",
    "\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_class_train_scaled = scaler.fit_transform(X_class_train)\n",
    "X_class_test_scaled = scaler.transform(X_class_test)\n",
    "X_reg_train_scaled = scaler.fit_transform(X_reg_train)\n",
    "X_reg_test_scaled = scaler.transform(X_reg_test)\n",
    "\n",
    "# подбор гиперпараметров для логистической регрессии\n",
    "param_grid_class = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_search_class = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_class, scoring='f1_weighted', cv=5)\n",
    "grid_search_class.fit(X_class_train_scaled, y_class_train)\n",
    "print(f\"Лучший параметр C для логистической регрессии: {grid_search_class.best_params_}\")\n",
    "\n",
    "# подбор гиперпараметров для линейной регрессии\n",
    "param_grid_reg = {'fit_intercept': [True, False]}\n",
    "grid_search_reg = GridSearchCV(LinearRegression(), param_grid_reg, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "print(f\"Лучший параметр fit_intercept для линейной регрессии: {grid_search_reg.best_params_}\")\n",
    "\n",
    "log_reg_best = grid_search_class.best_estimator_\n",
    "log_reg_best.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred_best = log_reg_best.predict(X_class_test_scaled)\n",
    "\n",
    "lin_reg_best = grid_search_reg.best_estimator_\n",
    "lin_reg_best.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred_best = lin_reg_best.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"\\nМетрики для классификации (улучшенный бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_best):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_best, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (улучшенный бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_best):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации (собственная реализация):\n",
      "Accuracy: 0.88\n",
      "F1-Score: 0.83\n",
      "\n",
      "Метрики для регрессии (собственная реализация):\n",
      "MSE: 6.57\n",
      "MAE: 2.07\n",
      "R² Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionFromScratch:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, regularization=None, reg_lambda=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.regularization = regularization\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            if self.regularization == 'l2':\n",
    "                dw += (self.reg_lambda / n_samples) * self.weights\n",
    "            elif self.regularization == 'l1':\n",
    "                dw += (self.reg_lambda / n_samples) * np.sign(self.weights)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "class LogisticRegressionFromScratch:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, regularization=None, reg_lambda=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.regularization = regularization\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            if self.regularization == 'l2':\n",
    "                dw += (self.reg_lambda / n_samples) * self.weights\n",
    "            elif self.regularization == 'l1':\n",
    "                dw += (self.reg_lambda / n_samples) * np.sign(self.weights)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "\n",
    "log_reg_scratch = LogisticRegressionFromScratch(learning_rate=0.01, epochs=2000, regularization='l2', reg_lambda=0.1)\n",
    "log_reg_scratch.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred_scratch = log_reg_scratch.predict(X_class_test_scaled)\n",
    "\n",
    "lin_reg_scratch = LinearRegressionFromScratch(learning_rate=0.01, epochs=2000, regularization='l2', reg_lambda=0.1)\n",
    "lin_reg_scratch.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred_scratch = lin_reg_scratch.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"Метрики для классификации (собственная реализация):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_scratch):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_scratch, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (собственная реализация):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_scratch):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_scratch):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_scratch):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №3 (Проведение исследований с решающим деревом) \n",
    "\n",
    "**3a. Формулирование гипотез**\n",
    "1) Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации (бейзлайн):\n",
      "Accuracy: 0.96\n",
      "F1-Score: 0.96\n",
      "\n",
      "Метрики для регрессии (бейзлайн):\n",
      "MSE: 120.64\n",
      "MAE: 8.30\n",
      "R² Score: -1.40\n",
      "Лучшие параметры для классификации: {'max_depth': 5, 'min_samples_split': 2}\n",
      "Лучшие параметры для регрессии: {'max_depth': 3, 'min_samples_split': 10}\n",
      "\n",
      "Метрики для классификации (улучшенный бейзлайн):\n",
      "Accuracy: 0.97\n",
      "F1-Score: 0.96\n",
      "\n",
      "Метрики для регрессии (улучшенный бейзлайн):\n",
      "MSE: 60.17\n",
      "MAE: 6.56\n",
      "R² Score: -0.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex'])  # 0 - Female, 1 - Male\n",
    "data['BP'] = label_encoder.fit_transform(data['BP'])    # 0 - HIGH, 1 - LOW, 2 - NORMAL\n",
    "data['Cholesterol'] = label_encoder.fit_transform(data['Cholesterol'])  # 0 - HIGH, 1 - NORMAL\n",
    "data['Drug'] = label_encoder.fit_transform(data['Drug'])\n",
    "\n",
    "X_class = data[['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']]\n",
    "y_class = data['Drug']\n",
    "X_reg = data[['Age', 'Sex', 'BP', 'Cholesterol']]\n",
    "y_reg = data['Na_to_K']\n",
    "\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_class_train, y_class_train)\n",
    "y_class_pred = clf.predict(X_class_test)\n",
    "\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_reg_train, y_reg_train)\n",
    "y_reg_pred = reg.predict(X_reg_test)\n",
    "\n",
    "print(\"Метрики для классификации (бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred):.2f}\")\n",
    "\n",
    "param_grid_class = {'max_depth': [3, 5, 10, None], 'min_samples_split': [2, 5, 10]}\n",
    "grid_search_class = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_class, scoring='f1_weighted', cv=5)\n",
    "grid_search_class.fit(X_class_train, y_class_train)\n",
    "\n",
    "param_grid_reg = {'max_depth': [3, 5, 10, None], 'min_samples_split': [2, 5, 10]}\n",
    "grid_search_reg = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid_reg, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "print(f\"Лучшие параметры для классификации: {grid_search_class.best_params_}\")\n",
    "print(f\"Лучшие параметры для регрессии: {grid_search_reg.best_params_}\")\n",
    "\n",
    "clf_best = grid_search_class.best_estimator_\n",
    "clf_best.fit(X_class_train, y_class_train)\n",
    "y_class_pred_best = clf_best.predict(X_class_test)\n",
    "\n",
    "reg_best = grid_search_reg.best_estimator_\n",
    "reg_best.fit(X_reg_train, y_reg_train)\n",
    "y_reg_pred_best = reg_best.predict(X_reg_test)\n",
    "\n",
    "print(\"\\nМетрики для классификации (улучшенный бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_best):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_best, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (улучшенный бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_best):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "F1-Score: 0.98\n",
      "MSE: 12.42107825428839\n",
      "MAE: 2.4547285606060614\n",
      "R² Score: 0.7524326215882193\n"
     ]
    }
   ],
   "source": [
    "class DecisionTreeFromScratch:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(sample, self.tree) for sample in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if len(y) < self.min_samples_split or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if not best_split:\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        left_tree = self._build_tree(X[best_split['indices_left']], y[best_split['indices_left']], depth + 1)\n",
    "        right_tree = self._build_tree(X[best_split['indices_right']], y[best_split['indices_right']], depth + 1)\n",
    "        return {\n",
    "            'feature_index': best_split['feature_index'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_split = None\n",
    "        best_score = float('inf') \n",
    "\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                indices_left = X[:, feature_index] <= threshold\n",
    "                indices_right = X[:, feature_index] > threshold\n",
    "                if len(indices_left) == 0 or len(indices_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                score = self._calculate_split_score(y[indices_left], y[indices_right])\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_split = {\n",
    "                        'feature_index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'indices_left': indices_left,\n",
    "                        'indices_right': indices_right\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_split_score(self, y_left, y_right):\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        if np.issubdtype(y_left.dtype, np.integer):\n",
    "            return (len(y_left) * self._gini(y_left) + len(y_right) * self._gini(y_right)) / (len(y_left) + len(y_right))\n",
    "        else:\n",
    "            return (len(y_left) * self._mse(y_left) + len(y_right) * self._mse(y_right)) / (len(y_left) + len(y_right))\n",
    "\n",
    "    def _gini(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities**2)\n",
    "\n",
    "    def _mse(self, y):\n",
    "        mean = np.mean(y)\n",
    "        return np.mean((y - mean)**2)\n",
    "\n",
    "    def _create_leaf(self, y):\n",
    "        if np.issubdtype(y.dtype, np.integer):\n",
    "            return np.bincount(y).argmax()\n",
    "        else:\n",
    "            return np.mean(y)\n",
    "\n",
    "    def _predict_sample(self, sample, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            feature_value = sample[tree['feature_index']]\n",
    "            if feature_value <= tree['threshold']:\n",
    "                return self._predict_sample(sample, tree['left'])\n",
    "            else:\n",
    "                return self._predict_sample(sample, tree['right'])\n",
    "        else:\n",
    "            return tree\n",
    "\n",
    "clf = DecisionTreeFromScratch(max_depth=5, min_samples_split=2)\n",
    "clf.fit(X_class_train.values, y_class_train.values)\n",
    "y_pred = clf.predict(X_class_test.values)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_class_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_class_test, y_pred, average='weighted'))\n",
    "\n",
    "reg = DecisionTreeFromScratch(max_depth=5, min_samples_split=2)\n",
    "reg.fit(X_reg_train.values, y_reg_train.values)\n",
    "y_pred = reg.predict(X_reg_test.values)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_reg_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_reg_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_reg_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №4 (Проведение исследований со случайным лесом) \n",
    "\n",
    "**3a. Формулирование гипотез**\n",
    "1) Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации (бейзлайн):\n",
      "Accuracy: 0.89\n",
      "F1-Score: 0.88\n",
      "\n",
      "Метрики для регрессии (бейзлайн):\n",
      "MSE: 71.92\n",
      "MAE: 7.02\n",
      "R² Score: -0.43\n",
      "Лучшие параметры для классификации: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Лучшие параметры для регрессии: {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\n",
      "Метрики для классификации (улучшенный бейзлайн):\n",
      "Accuracy: 0.92\n",
      "F1-Score: 0.92\n",
      "\n",
      "Метрики для регрессии (улучшенный бейзлайн):\n",
      "MSE: 55.74\n",
      "MAE: 6.40\n",
      "R² Score: -0.11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_class_train, y_class_train)\n",
    "y_class_pred = clf.predict(X_class_test)\n",
    "\n",
    "reg = RandomForestRegressor(random_state=42)\n",
    "reg.fit(X_reg_train, y_reg_train)\n",
    "y_reg_pred = reg.predict(X_reg_test)\n",
    "\n",
    "print(\"Метрики для классификации (бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred):.2f}\")\n",
    "\n",
    "param_grid_class = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "param_grid_reg = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_class = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_class, scoring='f1_weighted', cv=5)\n",
    "grid_search_class.fit(X_class_train, y_class_train)\n",
    "\n",
    "grid_search_reg = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_reg, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "print(f\"Лучшие параметры для классификации: {grid_search_class.best_params_}\")\n",
    "print(f\"Лучшие параметры для регрессии: {grid_search_reg.best_params_}\")\n",
    "\n",
    "clf_best = grid_search_class.best_estimator_\n",
    "clf_best.fit(X_class_train, y_class_train)\n",
    "y_class_pred_best = clf_best.predict(X_class_test)\n",
    "\n",
    "reg_best = grid_search_reg.best_estimator_\n",
    "reg_best.fit(X_reg_train, y_reg_train)\n",
    "y_reg_pred_best = reg_best.predict(X_reg_test)\n",
    "\n",
    "print(\"\\nМетрики для классификации (улучшенный бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_best):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_best, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (улучшенный бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_best):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации (собственная реализация):\n",
      "Accuracy: 0.68\n",
      "F1-Score: 0.57\n",
      "\n",
      "Метрики для регрессии (собственная реализация):\n",
      "MSE: 54.73\n",
      "MAE: 6.31\n",
      "R² Score: -0.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "class RandomForestFromScratchClassifier:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, max_features=\"sqrt\", random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_sample, y_sample = resample(X, y, random_state=self.random_state)\n",
    "\n",
    "            if self.max_features == \"sqrt\":\n",
    "                n_sub_features = int(np.sqrt(n_features))\n",
    "            elif self.max_features == \"log2\":\n",
    "                n_sub_features = int(np.log2(n_features))\n",
    "            else:\n",
    "                n_sub_features = n_features\n",
    "\n",
    "            feature_indices = np.random.choice(n_features, n_sub_features, replace=False)\n",
    "\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, random_state=self.random_state)\n",
    "            tree.fit(X_sample[:, feature_indices], y_sample)\n",
    "            self.trees.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((len(X), len(self.trees)))\n",
    "\n",
    "        for i, (tree, feature_indices) in enumerate(self.trees):\n",
    "            predictions[:, i] = tree.predict(X[:, feature_indices])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=predictions)\n",
    "\n",
    "class RandomForestFromScratchRegressor:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, max_features=\"sqrt\", random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_sample, y_sample = resample(X, y, random_state=self.random_state)\n",
    "            if self.max_features == \"sqrt\":\n",
    "                n_sub_features = int(np.sqrt(n_features))\n",
    "            elif self.max_features == \"log2\":\n",
    "                n_sub_features = int(np.log2(n_features))\n",
    "            else:\n",
    "                n_sub_features = n_features\n",
    "\n",
    "            feature_indices = np.random.choice(n_features, n_sub_features, replace=False)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=self.random_state)\n",
    "            tree.fit(X_sample[:, feature_indices], y_sample)\n",
    "            self.trees.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((len(X), len(self.trees)))\n",
    "\n",
    "        for i, (tree, feature_indices) in enumerate(self.trees):\n",
    "            predictions[:, i] = tree.predict(X[:, feature_indices])\n",
    "        return np.mean(predictions, axis=1)\n",
    "\n",
    "\n",
    "rf_classifier = RandomForestFromScratchClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
    "rf_classifier.fit(X_class_train.values, y_class_train.values)\n",
    "y_class_pred = rf_classifier.predict(X_class_test.values)\n",
    "\n",
    "print(\"Метрики для классификации (собственная реализация):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred, average='weighted'):.2f}\")\n",
    "\n",
    "rf_regressor = RandomForestFromScratchRegressor(n_estimators=10, max_depth=5, random_state=42)\n",
    "rf_regressor.fit(X_reg_train.values, y_reg_train.values)\n",
    "y_reg_pred = rf_regressor.predict(X_reg_test.values)\n",
    "\n",
    "print(\"\\nМетрики для регрессии (собственная реализация):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №5 (Проведение исследований с градиентным бустингом) \n",
    "\n",
    "**3a. Формулирование гипотез**\n",
    "1) Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации (бейзлайн):\n",
      "Accuracy: 0.90\n",
      "F1-Score: 0.89\n",
      "\n",
      "Метрики для регрессии (бейзлайн):\n",
      "MSE: 70.63\n",
      "MAE: 6.78\n",
      "R² Score: -0.41\n",
      "Лучшие параметры для классификации: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Лучшие параметры для регрессии: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "\n",
      "Метрики для классификации (улучшенный бейзлайн):\n",
      "Accuracy: 0.92\n",
      "F1-Score: 0.91\n",
      "\n",
      "Метрики для регрессии (улучшенный бейзлайн):\n",
      "MSE: 52.78\n",
      "MAE: 6.25\n",
      "R² Score: -0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "clf.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred = clf.predict(X_class_test_scaled)\n",
    "\n",
    "reg = GradientBoostingRegressor(random_state=42)\n",
    "reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred = reg.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"Метрики для классификации (бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred):.2f}\")\n",
    "\n",
    "param_grid_class = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "param_grid_reg = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search_class = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid_class,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5\n",
    ")\n",
    "grid_search_class.fit(X_class_train_scaled, y_class_train)\n",
    "\n",
    "grid_search_reg = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_reg,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5\n",
    ")\n",
    "grid_search_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "\n",
    "print(f\"Лучшие параметры для классификации: {grid_search_class.best_params_}\")\n",
    "print(f\"Лучшие параметры для регрессии: {grid_search_reg.best_params_}\")\n",
    "\n",
    "clf_best = grid_search_class.best_estimator_\n",
    "clf_best.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred_best = clf_best.predict(X_class_test_scaled)\n",
    "\n",
    "reg_best = grid_search_reg.best_estimator_\n",
    "reg_best.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred_best = reg_best.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"\\nМетрики для классификации (улучшенный бейзлайн):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_best):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_best, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (улучшенный бейзлайн):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_best):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_best):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация алгоритма машинного обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики для классификации (реализация):\n",
      "Accuracy: 0.73\n",
      "F1-Score: 0.63\n",
      "\n",
      "Метрики для регрессии (реализация):\n",
      "MSE: 70.56\n",
      "MAE: 6.75\n",
      "R² Score: -0.41\n",
      "\n",
      "Метрики для классификации (улучшенная реализация):\n",
      "Accuracy: 0.75\n",
      "F1-Score: 0.64\n",
      "\n",
      "Метрики для регрессии (улучшенная реализация):\n",
      "MSE: 64.77\n",
      "MAE: 6.48\n",
      "R² Score: -0.29\n"
     ]
    }
   ],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, task=\"regression\"):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.task = task\n",
    "        self.models = []\n",
    "\n",
    "    def _gradient(self, y_true, y_pred):\n",
    "        if self.task == \"regression\":\n",
    "            return y_true - y_pred\n",
    "        elif self.task == \"classification\":\n",
    "            return y_true - 1 / (1 + np.exp(-y_pred))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        y_pred = np.zeros_like(y, dtype=float)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = self._gradient(y, y_pred)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.models.append(tree)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for tree in self.models:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        if self.task == \"classification\":\n",
    "            return (y_pred > 0).astype(int)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "gb_clf = GradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3, task=\"classification\")\n",
    "gb_clf.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred_impl = gb_clf.predict(X_class_test_scaled)\n",
    "\n",
    "gb_reg = GradientBoosting(n_estimators=100, learning_rate=0.1, max_depth=3, task=\"regression\")\n",
    "gb_reg.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred_impl = gb_reg.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"Метрики для классификации (реализация):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_impl):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_impl, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (реализация):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_impl):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_impl):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_impl):.2f}\")\n",
    "\n",
    "gb_clf_tuned = GradientBoosting(n_estimators=200, learning_rate=0.05, max_depth=5, task=\"classification\")\n",
    "gb_clf_tuned.fit(X_class_train_scaled, y_class_train)\n",
    "y_class_pred_tuned = gb_clf_tuned.predict(X_class_test_scaled)\n",
    "\n",
    "gb_reg_tuned = GradientBoosting(n_estimators=200, learning_rate=0.05, max_depth=5, task=\"regression\")\n",
    "gb_reg_tuned.fit(X_reg_train_scaled, y_reg_train)\n",
    "y_reg_pred_tuned = gb_reg_tuned.predict(X_reg_test_scaled)\n",
    "\n",
    "print(\"\\nМетрики для классификации (улучшенная реализация):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_class_test, y_class_pred_tuned):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_class_test, y_class_pred_tuned, average='weighted'):.2f}\")\n",
    "\n",
    "print(\"\\nМетрики для регрессии (улучшенная реализация):\")\n",
    "print(f\"MSE: {mean_squared_error(y_reg_test, y_reg_pred_tuned):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_reg_test, y_reg_pred_tuned):.2f}\")\n",
    "print(f\"R² Score: {r2_score(y_reg_test, y_reg_pred_tuned):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "1) **ЛР1 KNN** (K-Nearest Neighbors):\n",
    "\n",
    "- `Классификация`: Улучшенный бейзлайн и самостоятельная имплементация показывают одинаковые результаты (0.93), что указывает на стабильность алгоритма\n",
    "- `Регрессия`: Улучшение бейзлайна с 6.63 до 3.21, но самостоятельная имплементация дала результат хуже улучшенного бейзлайна (3.60)\n",
    "\n",
    "2) **ЛР2 Линейные модели:**\n",
    "\n",
    "- `Классификация`: Улучшенный бейзлайн (0.97) значительно превосходит самостоятельную имплементацию (0.88)\n",
    "- `Регрессия`: Все результаты близки (различия незначительные), что указывает на стабильность метода\n",
    "\n",
    "3) **ЛР3 Решающее дерево:**\n",
    "\n",
    "- `Классификация`: Результаты показывают улучшение в самостоятельной имплементации (0.98 против 0.96 для бейзлайна)\n",
    "- `Регрессия`: Существенное улучшение с 8.30 до 2.46 в самостоятельной имплементации, что подтверждает эффективность алгоритма\n",
    "\n",
    "4) **ЛР4 Случайный лес:**\n",
    "\n",
    "- `Классификация`: Самостоятельная имплементация дала наименьший результат (0.68), что указывает на неудачную настройку\n",
    "- `Регрессия`: Улучшение с 7.02 до 6.40, и самостоятельная имплементация не привела к значительным улучшениям (6.31) - удивительно\n",
    "\n",
    "5) **ЛР5 Градиентный бустинг:**\n",
    "\n",
    "- `Классификация`: Результаты улучшились с 0.90 до 0.92 в улучшенном бейзлайне, но самостоятельная имплементация дала хуже (0.75)\n",
    "- `Регрессия`: Улучшение с 6.78 до 6.25, но самостоятельная имплементация оказалась незначительно хуже (6.75)\n",
    "\n",
    "### Сравнение\n",
    "\n",
    "- `Лучшие результаты **по классификации**` продемонстрировали `решающее дерево (0.98)` и `линейные модели (0.97)`\n",
    "- `Лучшие результаты **по регрессии**` показало `решающее дерево (2.46)`\n",
    "- Самостоятельные имплементации в большинстве случаев дают более слабые результаты, чем улучшенные бейзлайны, особенно для случайного леса и градиентного бустинга (там получился вообще какой-то ужас)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
